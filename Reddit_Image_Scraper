#!/usr/bin/env perl

#   Reddit Image Scraper: A perl script to download images hosted on 
#   the imgur.com hosting service linked from a subreddit at reddit.com
#   Copyright (C) 2011 Joshua Hull

#   This program is free software: you can redistribute it and/or modify 
#   it under the terms of the GNU General Public License as published by
#   the Free Software Foundation, either version 3 of the License, or
#   (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program.  If not, see <http://www.gnu.org/licenses/>.

use strict;
use warnings;
use JSON;
use WWW::Mechanize;

die "Usage: $0 subreddit [...]\n" unless @ARGV;

foreach my $arg (@ARGV) {

    # Create the directory if necessary.
    mkdir $arg unless -d $arg;

    my $url = "http://www.reddit.com/r/$arg/.json?limit=1000";
    print "Downloading from http://www.reddit.com/r/$arg\n";
    my $mech = WWW::Mechanize->new;
    $mech->get($url);
    my $json_string = $mech->text();

    my $json = JSON->new;
    my $json_text = $json->allow_nonref->utf8->relaxed->decode($json_string);
    my $posts = $json_text->{data}->{children}; 
    foreach my $post (@{$posts}) {
        my $domain = $post->{data}->{domain};
        my $url = $post->{data}->{url};
        if ( $domain =~ m/i.imgur.com/){
            my $file_name = substr $url, -9;
	    if (-f "$arg/$file_name"){
	    } else {
            	print "      Downloading $url\n";
            	$mech->get($url,':content_file' => "$arg/$file_name");
            	print "      Done.\n";
	    }
        }
    }
}
