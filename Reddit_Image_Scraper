#!/usr/bin/env perl

#   Reddit Image Scraper: A perl script to download images hosted on 
#   the imgur.com hosting service linked from a subreddit at reddit.com
#   Copyright (C) 2011 Joshua Hull

#   This program is free software: you can redistribute it and/or modify 
#   it under the terms of the GNU General Public License as published by
#   the Free Software Foundation, either version 3 of the License, or
#   (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program.  If not, see <http://www.gnu.org/licenses/>.

#use strict;
use warnings;
use JSON;
use WWW::Mechanize;
use Tk;

my $subbreddit;

my $window = new MainWindow;
my $status_string;
my $status = $window -> Label(-textvariable => \$status_string) -> pack();
my $download_button = $window -> Button(-text => "DOWNLOAD!", -command =>\&download_subbreddit_dummy) -> pack();
my $subbreddit_entry = $window ->Entry() -> pack();
my $file_open = $window -> Button(-text => "Open Files...",-command =>\&open_file_dialog)-> pack();
$status_string = "Ready...";

MainLoop;

sub download_subbreddit_dummy {
	my $subbreddit = $subbreddit_entry -> get();
	$status_string = "Downloading from $subbreddit...";
}

sub open_file_dialog {
	$window->getOpenFile;
}

sub download_subbreddit {
    my $subbreddit = $subbreddit_entry -> get(); 
    mkdir $subbreddit unless -d $subbreddit;
    my $url = "http://www.reddit.com/r/$subbreddit/.json?limit=1000";
    print "Downloading from http://www.reddit.com/r/$subbreddit\n";
    my $mech = WWW::Mechanize->new(autocheck=>0);
    $mech->get($url);

    my $json_text = JSON->new->allow_nonref->utf8->relaxed->decode($mech->text);
    my $posts = $json_text->{data}->{children};
    foreach my $post (@{$posts}) {
        my $url = $post->{data}->{url};
        next unless $domain =~ m/i.imgur.com/;
        my $file_name = substr $url, -9;
        next unless !(-f "$subbreddit/$file_name");
        print "      Downloading $url\n";
        $mech->get($url,':content_file' => "$subbreddit/$file_name");
        print "      Done.\n";
        }
}

sub download_image {
    my ( $url, $arg ) = @_;
    my ($file_name) = ($url =~ m@imgur\.com/(?:a/)?(.+)$@i);
    ( -e "$arg/$file_name" )
      ? print "     Skipping $url (copy found)\n"
      : print "     Downloading $url as $file_name\n";
    $mech->get( $url, ':content_file' => "$arg/$file_name" ) || return;
    print "      Done.\n";
}

sub scrape_album {
    my ( $url, $arg ) = @_;
    print "     Found album $url\n";

    $mech->get($url);
    if ( my ($json) = ( $mech->content =~ m@images:\s+(.+?),\s*cover:@is ) ) {
        my $struct = JSON->new->allow_nonref->utf8->relaxed->decode($json);
        map {
            download_image( 'http://imgur.com/' . $_->{hash} . $_->{ext}, $arg )
        } @{ $struct->{items} };
    }
    else {    #otherwise, we've just found a page like http://imgur.com/aOLy2
        my ($img) = ($url =~ m@imgur\.com/([^\.#]+)@i);
        return unless $img;
        $mech->follow_link( url_regex => qr/$img/i );
        download_image( $mech->uri, $arg );
    }
}
